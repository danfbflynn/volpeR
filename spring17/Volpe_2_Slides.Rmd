---
title: 'Volpe R Course: Session 2'
author: "| Instructors: Don Fisher, Dan Flynn, Jessie Yang \n| Course webpage: http://bit.ly/volpeR\n"
date: "3/30/2017"
output:
  beamer_presentation:
    includes:
      in_header: beamertemp.tex
    incremental: no
  slidy_presentation: default
fontsize: 8pt
colortheme: dolphin
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(pander)
```


## Review 

Last session we covered essential material on

- Basic R operations
- Summarizing data; writing functions
- Getting data in 
- Indexing 

Today we will move to data analysis in R, but will aim to review some of that material as we go.



## Homework: Writing a function
### Excercise 1:
Write a function to calculate the minimum, median, and standard deviation of a vector, and show the result on the console.

Some possible solutions:
```{r}
summarize.1 <- function(x){
  print(min(x))
  print(median(x))
  print(sd(x))
  }

summarize.2 <- function(x){
  min.x <- min(x)
  med.x <- median(x)
  sd.x <- sd(x)
  print(data.frame(Min = min.x, Median = med.x, SD = sd.x))
}
```

## Homework: Writing a function
```{r}
x = seq(1, 100, by = 7)
summarize.1(x)
summarize.2(x)
```

## Homework: Writing a function
### Some of your answers!
```{r}
# This solution is more complex than we have time to discuss today!
sumstats = function(x) {
  str_out = sprintf("min:\t%s\nmedian:\t%s\nstdev:\t%s", 
                    min(x), median(x), sd(x))
  cat(str_out)
  }

# This solution is similar to summarize.1, but with added text
descriptive <- function(x) {
  cat("vector minimum: ", min(x), "\n")
  cat("vector median: ", median(x), "\n")
  cat("vector standard deviation: ", sd(x), "\n")
  }
```
---
```{r}
sumstats(x)
descriptive(x)
```
See `?sprintf` and `?cat` to understand these solutions. 

Lessons: 

- There are many ways to solve problems in R!
- `print()` and `cat()` can be used return output to the console

## The standard statistical toolbox

These tools are the heart of all statistical analysis. We are assuming you have encountered these before in other courses, but feel free to ask general questions.

Today we'll cover three types of standard tools:

- Comparing frequencies of events
    + Chi-square
- Assessing relationships between continuous variables
    + Correlation and regression
- Comparing means of groups of variables 
    + T-test and ANOVA

## Comparing frequencies of events

This is one of the simplest tests: are some events more common than others? The Chi-squared test is used to test independence of variables, as well as in goodness-of-fit measures.

Simple case:

```{r table1, echo=FALSE, message=FALSE, warnings=FALSE, results='asis'}
tabl <- "

| Class   | Number   |
|:-----------|---------:|
| Freshmen   | 32       |
| Sophomores | 28       |
| Juniors    | 20       |
| Seniors    | 21       |
"
pander(tabl)

```
We only have four data points; while we might suspect there is a trend in enrollment here, we can only do a simple Chi-squared test, which is calculated as follows:
\[ \chi^2= \sum_{i=1}^{n} \frac{(O_i - E_i)^2}{E_i} \]


## Comparing frequencies of events

In the given case, the expected value in each should be 0.25 x the total, 101 students = 25.25. We could manually calculate the Chi-squared statistic as follows:

```{r}
chidat <- data.frame(Class = 1:4, Number = c(32, 28, 20, 21))
(E <- sum(chidat$Number)/nrow(chidat))
# Manual calculation
(32 - E)^2 / E + (28 - E)^2 / E + 
  (20 - E)^2 / E + (21 - E)^2 / E

chisq.test(chidat$Number)

```
<!-- This is a very simple case, because expected values are identical for all four cells, and this is essentially a single-row contingency table -->

## Comparing frequencies of events
In the case that the expected values are not identical, you can specify that in the `chisq.test` function. 
For example, perhaps the course is an upper-level course, and freshmen are expected to be only 15% of the registered students. We can specify the expected proportion as a vector `p`

```{r}
chisq.test(chidat$Number, p = c(0.15, 0.25, 0.30, 0.30))
```

### Exercise 2.1:
Look at the built in R data file `mtcars`, which we looked at last time.
Use the `table` function to look at the frequency of cars with different number of cylinders in the engines, variable `cyl`.
Carry out a Chi-squared test to see if the frequencies statistically different from equal. What do you conclude?


## Exercise 2.1
```{r, fig.width= 4, fig.height= 4}
cylfreq <- table(mtcars$cyl)
chisq.test(cylfreq)
par(cex = 0.5)
barplot(cylfreq, xlab = "Number of Cylinders", ylab = "Frequency")
abline(h = sum(cylfreq)/3, lwd = 2, col = "tomato")
```

## Relationship between continuous variables
Load the Boston housing price data (already in R). These are from a 1978 research paper.

```{r, echo = TRUE, results = 'hide'}
library(MASS)
summary(Boston)
```
There are two ways to think about continuous variables: correlation and regression.

Examine the correlation between crime rates and median value of owner-occupied houses:
```{r, results = "hide", fig.height = 3}
cor(Boston$crim, Boston$medv)
plot(Boston$crim, Boston$medv, cex = 0.75)
```

`plot(medv ~ crim, data = Boston)` Would be another option

## Relationship between continuous variables

We also often want to know if this relationship is different from what we would expect by chance:

```{r}
cor.test(Boston$crim, Boston$medv)
```

## Regression
For regression, we are assuming a causal relationship between the variables. Correlation only gives one output, the strength of the relationship expressed as `r`. 
For regression, we use workhorse of R analysis, `lm`:
```{r}
house.crime <- lm(medv ~ crim, data = Boston)
summary(house.crime)
```
## Regression
There is a lot to unpack with these results. Recall that regression minimizes the sum of squares between the observed values and the expected values. This method is called *Ordinary Least Squares (OLS)*, finding the slope coefficient which best matches the predictor and response variables.

The key values to look at in this case are the **Estimate** for the predictor variable, `crim`, as well as for the intercept. We also want to know if these results are significantly different from the null hypothesis of no relationship.
```{r}
coef(house.crime)
# And the p-value:
summary(house.crime)$coefficients[2,4]
```
## Regression
We also want to know if this is a good model. Let's look at the measure of goodness of fit. This should look familiar!
```{r, results='hide'}
summary(house.crime)$r.squared
cor(Boston$crim, Boston$medv)^2
```
Let's also plot the residuals. You can also use plot(house.crime) for a lot of output:
```{r, eval=FALSE, echo = TRUE}
plot(resid(house.crime))
```
```{r, results = 'hide', echo = FALSE, fig.height=3, fig.width=4, fig.align='center'}
plot(resid(house.crime), pch = 21, xlab = "", cex.axis=0.4, cex.lab = 0.45, cex = 0.3)
```

## Regression
Those look acceptable. If we plot the model with `plot(house.crime)` we see the Q-Q plot. The Q-Q plot should mostly be straight, as it is. If we are very concerned with normality of residuals, you can log-transform the response (most common) or predictors. Note that the interpretation of the coefficients is then different.

```{r, fig.height=3, fig.width=4}
hist(resid(house.crime))
# qqnorm(resid(house.crime)) # want this to be roughly straight line.
```

## Regression
Look at how average NOx concentrations in an area affect median housing prices:

```{r}
nox.house <- lm(medv ~ nox, data = Boston)
summary(nox.house)
```
## Regression
Plot it:
```{r, fig.height=3.5}
plot(medv ~ nox, 
     pch = 16, 
     col = "lightgrey",
     ylab = "Median House Value in $1000",
     xlab = "NOx concentration (parts per 10 million)",
     data = Boston)

# Add the trendline using "a B line" abline()
abline(nox.house)

```

## Regression 
We can use these coefficients to test scenarios. For example what if NOx concentrations are reduced by half?


1/2 x mean NOx concentration x slope of relationship + intercept.

```{r}
half.nox.price <- 0.5 * mean(Boston$nox) * 
  nox.house$coefficients[2] + nox.house$coefficients[1]
current.nox.price <- mean(Boston$medv)

# Print out on the console the expected housing prices 
# under the half NOx scenario and 'current'
paste("$",1000*round(c(half.nox.price, current.nox.price), 2), sep="")
```

## Multiple regression

Multiple regression is deceptively easy in R -- just add more variables!
Let's conduct a multiple regression with both NOx concentration and mean # of rooms
```{r}
multiple1 <- lm(medv ~ nox + rm, data = Boston)
summary(multiple1)

```
## Multiple regression

### Exercise 2.2:
Carry out a multiple regression for median house price and two predictor variables, using the Boston data set. See `?Boston` for a description of the variables. See if you can get a better model than the `nox` + `rm` model above.


## Exercise 2.2
How do we compare models? If we have the same response data in each model, we can compare using an index called AIC, Akaike Information Criterion. Of course you can look at $R^{2}$ as well, but there are pitfalls with only looking at $R^{2}$. 


```{r}
summary(multiple2 <- lm(medv ~ age + dis, data = Boston))
AIC(multiple1, multiple2)
```

Deciding how to compare models is a big topic in statistics. Your best approach is to clearly define the hypothesis you want to test and carry out the minimal number of statistical tests to address that hypothesis.

<!-- See an xy plot for each variable against the median housing value:
plot(medv ~ nox + rm + crim + zn + indus + chas + age + dis + rad +tax + ptratio + black + lstat) 
-->
<!--What about Kolmogorov-Smirnov Test:
look at housing prices for houses adjacent to the Charles river (chas = 1) or not:

```{r}
with(Boston, ks.test(medv[chas=="1"], medv[chas=="0"]))
```
-->

## T-test: compare means of two groups
Comparing two groups is done in R with the function `t.test`. This tests if two groups have the same mean value.
Here we are looking at housing prices for houses adjacent to the Charles river (`chas == 1`) or not:
```{r}
with(Boston, t.test(medv[chas=="1"], medv[chas=="0"]))
```
## T-Test: compare means of two groups
```{r, fig.height=4}
with(Boston, boxplot(medv[chas=="1"], 
                     medv[chas=="0"],
                     names = c("Adjacent to Charles R.",
                               "Not adjacent to Charles R."),
                     col = c("dodgerblue", "tomato"),
                     ylab = "Median home value (in $1000)"
                     )
     )
```

## Analysis of Variance (ANOVA): compare means of more than two groups

Analysis of variance is used for comparing means of more than two groups. Analysis of covariance (ANCOVA) combines features of ANOVA and regression. In fact, all of these models are related, in that they are linear models. A *generalized linear model* is the broadest category of these models.

```{r}
# create a three-category variable from the number of rooms per house

Boston$house.size <- cut(Boston$rm, 
                         breaks = c(0, 6, 6.8, 10),
                         labels = c("Small", "Medium", "Large"))

m1 <- aov(crim ~ house.size, data = Boston)
summary(m1)

```
## Analysis of Variance (ANOVA): compare means of more than two groups
Probably need to consider distributions here, as well!
```{r}
par(mfrow = c(1, 2))
boxplot(crim ~ house.size, data = Boston)
boxplot(crim ~ house.size, data = Boston, log = "y")

```

## Analysis of Variance (ANOVA): compare means of more than two groups

Do we need to transform the data?
As for the regression models above, the important point is that the *residuals of the model need to be normally distributed*. It is not important if the input data themselves are normal or not.

```{r, fig.height=4}
par(mfrow = c(1,2))
hist(resid(m1))
qqnorm(resid(m1)) # Not great!
```

## Analysis of Variance (ANOVA): compare means of more than two groups
```{r, fig.height=4}
m2 <- aov(log(crim) ~ house.size, data = Boston)
summary(m2)
par(mfrow = c(1,2))
hist(resid(m2))
qqnorm(m2$residuals) # Much better!
```
<!--

plot(m2$fitted, m2$res,
     xlab="Fitted",
     ylab="Residuals")
## Comparing lm, glm, aov

```{r}

# Compare this output with lm version of the same model

m3 <- lm(log(crim) ~ house.size, data = Boston)
summary(m3)
summary.aov(m3)

```
Summary of the lm shows how levels differ from an 'Intercept' = first level of a factor; will be chosen alphabetically if not expressly established.

```{r}
mean(log(Boston$crim)[Boston$house.size=="Small"])
```
Now look at the mean values for each house size
```{r}
mean.log.crim <- tapply(log(Boston$crim),
        Boston$house.size,
        mean)
diff(mean.log.crim)   
        
```
Our model estimates are different from these simple calculations, because of sample size differences. 

```{r}
coef(m3)

exp(coef(m3))

```
-->

## Homework

Take your dataset you used for Homework 1, or another dataset if you decide to change, and carry out at least two statistical tests.

Document your work in a script as before, and upload to the Homework 2 folder on the course Google Drive.

The more different things you try, the more feedback you will get from us!
